{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## üì¶ Overview for `embed_bge_m3.py`",
   "id": "afa0a8a15020e9e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This script loads a text embedding model, reads node data from a JSONL graph data export, computes vector embeddings in\n",
    "batches, and writes the output to a 'JSONL' file for use in retrieval-augmented generation (RAG).\n",
    "\n",
    "The 'JSONL' file is generated by `neo4j_exporter.py` and contains pre-extracted node data (ID, text, and labels)\n",
    "from a Neo4j graph. This avoids the need for a live Neo4j connection at embedding time.\n",
    "\n",
    "All configuration values such as model type, batch size, normalization, output file path, and logging are read from\n",
    "`digitaiCore/config.yaml`"
   ],
   "id": "6cddea9a20a315f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### üß∞ Standard Library Modules\n",
    "\n",
    "- `os`  Handles file paths and directory operations (e.g., joining paths, checking if files exist).\n",
    "\n",
    "- `json`  Reads and writes JSON and JSONL files (e.g., for embeddings or ID maps).\n",
    "\n",
    "- `time`  Used to track execution duration (e.g., embedding time for logging or profiling).\n",
    "\n",
    "- `logging`  Outputs progress, errors, and debug messages to a log file for monitoring and troubleshooting.\n",
    "\n",
    "---\n",
    "\n",
    "### ü§ñ AI & NLP Modules\n",
    "\n",
    "- `torch`\n",
    "  PyTorch deep learning framework ‚Äî required to run the BGE-M3 embedding model (supports GPU, CPU, or MPS on macOS).\n",
    "\n",
    "- `sentence_transformers.SentenceTransformer`\n",
    "  High-level API to load and run sentence-level embedding models like BGE-M3.\n",
    "  Converts input text into numerical vector representations used for similarity search.\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Project-Specific Module\n",
    "\n",
    "- `digitaiCore.config_loader.ConfigLoader`\n",
    "  Loads configuration from `config.yaml` with dot-notation access.\n",
    "  Used to retrieve paths, model settings, batch sizes, and logging options consistently across the project."
   ],
   "id": "4edfae0a1e54be4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import json\n",
    "import üíø\n",
    "import torch\n",
    "import logging\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from digitaiCore.config_loader import ConfigLoader"
   ],
   "id": "c00cdc9632a22adf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9c855bb7c0d2e4d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Initialization",
   "id": "e71d731b26ab8e5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Config üíø",
   "id": "97360c6a706116d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T12:53:06.466268Z",
     "start_time": "2025-07-14T12:53:06.464329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")) #Set path of root\n",
    "config_path = os.path.join(repo_root, \"digitaiCore\", \"config.yaml\") #Set path of config.yaml\n",
    "config = ConfigLoader(config_path) #Load in config"
   ],
   "id": "35ef41c2be34e9d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Set Performance Parameters üõ†Ô∏è\n",
    "### Controls the number of threads used by PyTorch and the intervals for Hugging Face tokenizer\n",
    "- This helps prevent CPU overloading üî• or stalls üò¥ during parallel batch encoding."
   ],
   "id": "d7d90e94008d9470"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch.set_num_threads(config.get(\"performance.num_threads\"))\n",
    "torch.set_num_interop_threads(config.get(\"performance.interop_threads\"))\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = str(config.get(\"performance.tokenizers_parallelism\")).lower()"
   ],
   "id": "55dcafce05dd88cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Set-up Logging  üìù\n",
    "Logging enable and log file location are controlled via parameters set in the 'config.yaml' file\n",
    "- Logging is enabled by default and is HIGHLY suggested\n",
    "    - Logging tracks batch processing progress along with where errors have occured\n",
    "        - Due to the immense line count in the embedding file it is incredibly hard to find errors by hand. Logging makes verification/diagnosis possible\n"
   ],
   "id": "fbad69e253049c1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if config.get(\"logging.enabled\"):\n",
    "    log_path = os.path.join(repo_root, config.get(\"logging.bgem3Log\"))  # Uses bgem3Log from config\n",
    "    os.makedirs(os.path.dirname(log_path), exist_ok=True)  # Ensure the logs/ directory exists\n",
    "\n",
    "    logging.basicConfig(\n",
    "        filename=log_path,\n",
    "        level=getattr(logging, config.get(\"logging.level\")),\n",
    "        format=config.get(\"logging.format\")\n",
    "    )\n",
    "    logging.info(\"=== Embedding Script Start ===\")\n"
   ],
   "id": "ed86cb91c2dede57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9769d421009879c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# üêç Breaking Down the Code:",
   "id": "3f1323105213b26f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load in SentenceTransformer Model\n",
    "The embedding model specicified in the config.yaml file will be used to convert natural language into numerical vectors.\n",
    "- In our case the embedding model is `BGE-M3`"
   ],
   "id": "9b9625e499d76d99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_name = config.get(\"embedding.model\")\n",
    "try:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    if config.get(\"logging.enabled\"):\n",
    "        logging.info(f\"Loaded model: {model_name}\")\n",
    "except Exception as e:\n",
    "    logging.exception(\"Model load failed\")\n",
    "    raise SystemExit(f\"[FATAL] Could not load model: {e}\")"
   ],
   "id": "f1a613dbe0c52378"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Node Texts from JSONL Neo4j Export\n",
    "This replaces the need for querying Neo4j for every piece each time the embedding model runs\n",
    "- Instead we use a static export file created by `neo4j_exporter.py`\n",
    "Each line in the file must be a JSON object with `id`, `text`, and optional `labels`"
   ],
   "id": "5a5cb807a717f173"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load üíø and validate ‚úÖ neo4j jsonl using path specified in config",
   "id": "1fd1c723f0fe9ed7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "input_path = os.path.join(repo_root, config.get(\"dataPaths.neo4jExport\"))\n",
    "if not os.path.exists(input_path):\n",
    "    raise SystemExit(f\"[FATAL] Input file not found: {input_path}\")"
   ],
   "id": "8b76f4c6e9ae7913"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loop through all nodes reading and making embeddings\n",
    "- Ensure all nodes contain actual body data; structural nodes are skipped\n",
    "    - Logging is displayed/output if enabled"
   ],
   "id": "5560f2217fd96c55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nodes = []\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        # Only embed nodes that contain actual body text; structure-only nodes are skipped\n",
    "        if record.get(\"text\"):\n",
    "            nodes.append((record[\"id\"], record[\"text\"]))\n",
    "\n",
    "if config.get(\"logging.enabled\"):\n",
    "    logging.info(f\"Loaded {len(nodes)} nodes with text from {input_path}\")"
   ],
   "id": "cc8181187ae7a0b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### If no nodes exist a failure is output and the file stops\n",
    "- This prevents wasting CPU/GPU resources or generating an empty output file"
   ],
   "id": "6356a5d606722931"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if not nodes:\n",
    "    if config.get(\"logging.enabled\"):\n",
    "        logging.error(\"No text nodes found in the input file. Cannot proceed.\")\n",
    "    raise SystemExit(\"[FATAL] No text nodes found in the input JSONL file. Check your export or path.\")"
   ],
   "id": "f62ba243d1cb1f86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize Embedding Model",
   "id": "b54410f5272fdfd4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Read embedding settings from config.yaml and resolve output file path to avoid overwriting",
   "id": "2b302d3d940c3e4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# These are controlled through config.yaml and affect batching, normalization, and resource pacing.\n",
    "batch_size = config.get(\"embedding.batch_size\")     # Number of documents per batch\n",
    "normalize = config.get(\"embedding.normalize\")       # If True, performs L2 normalization (cosine similarity prep)\n",
    "throttle = config.get(\"embedding.throttle\")         # Optional pause between batches (in seconds)\n",
    "\n"
   ],
   "id": "900212323c4f4531"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Resolve Output File Paths\n",
    "- This avoids overwriting the original neo4j export and allows that dataset to be embedded more than once if needed"
   ],
   "id": "177b42250f9a3e5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_path = os.path.join(repo_root, config.get(\"dataPaths.bgem3Embeddings\"))\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "print(f\"[DEBUG] Writing embeddings to: {output_path}\")"
   ],
   "id": "25bafa8ded0cd6ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Run Embedding Model\n",
    "- Text is embedded in batches for efficiency (batch size is defined in the config file)\n",
    "- For each batch of node texts, compute scentence embeddings\n",
    "    - Write each result to a new JSONL line\n",
    "\n",
    "`model.encode` generates embeddings using the SentenceTransformer model and returns a NumPy array which we convert to lists for JSON serialization.\n"
   ],
   "id": "c0884062156ed332"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(0, len(nodes), batch_size):\n",
    "        batch = nodes[i:i + batch_size]\n",
    "        ids, texts = zip(*batch)\n",
    "\n",
    "        try:\n",
    "            batch_embeddings = model.encode(\n",
    "                list(texts),\n",
    "                batch_size=batch_size,\n",
    "                convert_to_numpy=True,\n",
    "                normalize_embeddings=normalize,\n",
    "                show_progress_bar=True\n",
    "            )"
   ],
   "id": "cfbd0e9a15ffdf39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Write each node ID and its embedding to output file immediately (Avoids memory buildup)\n",
    "- Embedding error handling:\n",
    "    - Step 1: Write error to log (Optional depending on log enable)\n",
    "    - Step 2: Skip to the next batch\n",
    "**NOTE** In the case in which theres an error the entire rest of that batch will be skipped\n",
    "<br>\n",
    "\n",
    "- Throttle (Optional)\n",
    "    - At end of each batch pause for a set amount of time\n",
    "        - Manages memory and system load on slower machines"
   ],
   "id": "44f6c0e616a66b54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "            for node_id, emb in zip(ids, batch_embeddings):\n",
    "                json.dump({\"id\": node_id, \"embedding\": emb.tolist()}, f)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "            if config.get(\"logging.enabled\"):\n",
    "                logging.info(f\"Embedded batch {i} to {i + len(batch)}\")\n",
    "        except Exception as e:\n",
    "            if config.get(\"logging.enabled\"):\n",
    "                logging.error(f\"Embedding failed for batch {i} to {i + len(batch)}: {e}\")\n",
    "            continue\n",
    "\n",
    "            time.sleep(throttle)"
   ],
   "id": "bc8fc61f1d11c401"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (If Logging Enabled) Log the script completion and file path to confirm success",
   "id": "e08352ff6cad60b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if config.get(\"logging.enabled\"):\n",
    "    logging.info(f\"Saved embeddings to {output_path}\")\n",
    "    logging.info(\"=== Embedding Script End ===\")\n",
    "\n",
    "print(\"‚úÖ Embedding complete.\")"
   ],
   "id": "b3880f488b9d4226"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
